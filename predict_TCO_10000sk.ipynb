{
 "metadata": {
  "name": "",
  "signature": "sha256:6cc74ff3d280d22111f969e2a7fa37c91e7621f5ef3f6a462653befe62c29951"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import LinearRegression \n",
      "from collections import Counter\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "from pprint import pprint\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import math\n",
      "import glob\n",
      "import re\n",
      "import time\n",
      "\n",
      "#sentiment afinn\n",
      "afinn_file = open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])\n",
      "\n",
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            #print '\\t%s=%d' % (t, afinn[t])\n",
      "            total += afinn[t]\n",
      "    return total\n",
      "\n",
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += -1 * afinn[t]\n",
      "    return pos, neg\n",
      "\n",
      "start_time = time.time()\n",
      "filename = \"cars\"\n",
      "\n",
      "def read_automobiles(filename):\n",
      "    \"\"\" Read a list of automobiles.\n",
      "    Args:\n",
      "      filename: The name of the text file containing automobiles make model year.\n",
      "    Returns:\n",
      "      A list of automobile make model year\n",
      "    \"\"\"\n",
      "    automobiles = []\n",
      "    with open(filename) as f:\n",
      "        lines = f.read().splitlines()\n",
      "        for item in lines:\n",
      "            automobiles.append(item)  \n",
      "    return automobiles\n",
      "\n",
      "autos = read_automobiles(filename+'.txt')\n",
      "review_data = [] #X\n",
      "price_data = [] #y\n",
      "price_data_linear = []#y linear\n",
      "sentiment_data = [] #sentiment\n",
      "sentiment_data_linear = [] #sentiment linear\n",
      "\n",
      "#collect data\n",
      "for auto in autos:\n",
      "    filelist = []\n",
      "    filelist = glob.glob(filename + \"_review/\"+auto+\"/*.txt\")\n",
      "    tcofile = glob.glob(filename + \"_TCO/\" + filename + \"_TCO.txt\")\n",
      "    for fil in filelist:\n",
      "        with open(tcofile[0],'r') as file2:\n",
      "            with open(fil,'r') as file1:\n",
      "                price = 0\n",
      "                linear_price = 0\n",
      "                for line in file2:\n",
      "                    if line.split()[3] == fil.split(\"/\")[2][:9]:\n",
      "                        if float(line.split()[4]) >= 0 and float(line.split()[4])<10000:\n",
      "                            price = \"0-10000\"\n",
      "                            linear_price = 5000\n",
      "                        if float(line.split()[4]) >= 10000 and float(line.split()[4])<20000:\n",
      "                            price = \"10000-20000\"\n",
      "                            linear_price = 15000\n",
      "                        if float(line.split()[4]) >= 20000 and float(line.split()[4])<30000:\n",
      "                            price = \"20000-30000\"\n",
      "                            linear_price = 25000\n",
      "                        if float(line.split()[4]) >= 30000 and float(line.split()[4])<40000:\n",
      "                            price = \"30000-40000\"\n",
      "                            linear_price = 35000\n",
      "                        if float(line.split()[4]) >= 40000 and float(line.split()[4])<50000:\n",
      "                            price = \"40000-50000\"\n",
      "                            linear_price = 45000\n",
      "                        if float(line.split()[4]) >= 50000 and float(line.split()[4])<60000:\n",
      "                            price = \"50000-60000\"\n",
      "                            linear_price = 55000\n",
      "                        if float(line.split()[4]) >= 60000 and float(line.split()[4])<70000:\n",
      "                            price = \"60000-70000\"\n",
      "                            linear_price = 65000\n",
      "                        if float(line.split()[4]) >= 70000 and float(line.split()[4])<80000:\n",
      "                            price = \"70000-80000\"\n",
      "                            linear_price = 75000\n",
      "                        if float(line.split()[4]) >= 80000 and float(line.split()[4])<90000:\n",
      "                            price = \"80000-90000\"\n",
      "                            linear_price = 85000\n",
      "                        if float(line.split()[4]) >= 90000 and float(line.split()[4])<100000:\n",
      "                            price = \"90000-100000\"\n",
      "                            linear_price = 95000\n",
      "                        if float(line.split()[4]) >= 100000:\n",
      "                            price = \">100000\"\n",
      "                            linear_price = 100000\n",
      "\n",
      "                        reviews = file1.read().split('\\n------------------------------------------------------------------------------------------\\n')\n",
      "                        reviews = filter(None,reviews)\n",
      "                        for i in range(0,len(reviews)):\n",
      "                            review_data.append(reviews[i].replace(\"\\n\",\"\").decode('UTF-8'))\n",
      "                            price_data_linear.append(linear_price)\n",
      "                            price_data.append(price)\n",
      "                            sentiment_data.append(afinn_sentiment(reviews[i].replace(\"\\n\",\"\").decode('UTF-8').split(), afinn))\n",
      "                            sentiment_data_linear.append(afinn_sentiment2(reviews[i].replace(\"\\n\",\"\").decode('UTF-8').split(), afinn))\n",
      "                            \n",
      "print 'sentiment vector1:',Counter(sentiment_data).items()\n",
      "print 'price labels:', Counter(price_data).items()\n",
      "print 'linear price lables:',Counter(price_data_linear).items()\n",
      "\n",
      "\n",
      "def tokenize(reviews,vectorizer):\n",
      "    X = vectorizer.fit_transform(reviews)\n",
      "    print 'X dimensions=', X.shape\n",
      "    return X\n",
      "\n",
      "X = tokenize(review_data, TfidfVectorizer())\n",
      "print 'vector y dimensions:%d'%len(price_data)\n",
      "print 'sentiment vector dimensions:%d'%len(sentiment_data)\n",
      "\n",
      "def do_cv(X, y, nfolds=10):\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    return np.mean(cross_val_score(LogisticRegression(), X, y, cv=cv))\n",
      "\n",
      "print 'cross validation accuarcy(Logistic Regression):%.10f'%do_cv(X, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(min_df):'\n",
      "\n",
      "#how filtering rare words affect accuracy\n",
      "best_min_df = 0\n",
      "def compare_mindf(reviews, y):\n",
      "    accuracies = []\n",
      "    for freq in range(20):\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(min_df=freq)), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xlabel('mindf')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "    \n",
      "compare_mindf(review_data,price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(ngrams):'\n",
      "best_ngram_range = (1,1)\n",
      "def compare_ngrams(reviews, y):\n",
      "    accuracies = []\n",
      "    ngrams = [(1,1), (1,2), (2,2), (1,3), (2,3), (3,3), (3,4), (4,4)]\n",
      "    for ngram in ngrams:\n",
      "        print ngram\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(min_df=2, ngram_range=ngram)), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks(range(len(ngrams)), ngrams)\n",
      "    plt.xlabel('ngrams')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "   \n",
      "compare_ngrams(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(binary/freq):'\n",
      "best_binary = True\n",
      "def compare_binary(reviews, y):\n",
      "    accuracies = []\n",
      "    choices = [True, False]\n",
      "    for choice in choices:\n",
      "        print choice\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(binary=choice, min_df=2,ngram_range=(1,3))), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks([0,1], [str(c) for c in choices])\n",
      "    plt.xlabel('binary')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "        \n",
      "compare_binary(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(idf):'\n",
      "best_use_idf= True\n",
      "def compare_idf(reviews, y):\n",
      "    accuracies = []\n",
      "    choices = [True, False]\n",
      "    for choice in choices:\n",
      "        print choice\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(binary=True,min_df=2,ngram_range=(1,3),use_idf=choice)), y))\n",
      "\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks([0,1], [str(c) for c in choices])\n",
      "    plt.xlabel('binary')\n",
      "    plt.ylabel('use idf?')\n",
      "    plt.show()\n",
      "        \n",
      "compare_idf(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(filtering common words affect):'\n",
      "best_maxdf=1.\n",
      "def compare_maxdf(reviews, y):\n",
      "    accuracies = []\n",
      "    maxdfs = [1., .1, .05, .04, .03, .02, .01,.005,.001]\n",
      "    for freq in maxdfs:\n",
      "        print freq\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(max_df=freq)), y))\n",
      "        \n",
      "    plt.figure()\n",
      "    plt.plot(maxdfs, accuracies, 'bo-')\n",
      "    plt.xlabel('maxdf (fraction)')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "        \n",
      "compare_maxdf(review_data, price_data)\n",
      "\n",
      "print 'GridSearch on all parameters:'\n",
      "pipeline = Pipeline([\n",
      "    ('vect', TfidfVectorizer()),\n",
      "    ('clf', LogisticRegression()),\n",
      "])\n",
      "\n",
      "parameters = {\n",
      "    'vect__min_df': (1, 2, 3, 4, 5),\n",
      "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams or trigrams\n",
      "    'vect__use_idf': (True, False),\n",
      "}\n",
      "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=KFold(len(price_data), 10))\n",
      "print \"Performing grid search...\"\n",
      "print \"pipeline:\", [name for name, _ in pipeline.steps]\n",
      "print \"parameters:\"\n",
      "pprint(parameters)\n",
      "grid_search.fit(review_data, price_data)\n",
      "print \"done.\"\n",
      "\n",
      "print(\"Best accuarcy: %0.3f\" % grid_search.best_score_)\n",
      "print(\"Best parameters set:\")\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "print \"\\nRunning time is %s seconds\"%(time.time()-start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "sentiment vector1: [(0.0, 333), (1.0, 262), (2.0, 387), (3.0, 421), (4.0, 379), (5.0, 364), (6.0, 315), (7.0, 275), (8.0, 277), (9.0, 220), (10.0, 203), (11.0, 142), (12.0, 170), (13.0, 139), (14.0, 92), (15.0, 81), (16.0, 74), (17.0, 50), (18.0, 33), (19.0, 31), (20.0, 36), (21.0, 19), (22.0, 12), (23.0, 9), (24.0, 11), (25.0, 12), (26.0, 7), (28.0, 2), (29.0, 2), (30.0, 2), (31.0, 3), (34.0, 1), (55.0, 1), (-11.0, 9), (-21.0, 1), (-17.0, 1), (-16.0, 1), (-15.0, 2), (-14.0, 4), (-13.0, 4), (-12.0, 8), (-2.0, 193), (-10.0, 18), (-9.0, 22), (-8.0, 49), (-7.0, 59), (-6.0, 68), (-5.0, 65), (-4.0, 134), (-3.0, 150), (-1.0, 236)]\n",
        "price labels: [('0-10000', 8), ('50000-60000', 1013), ('>100000', 54), ('20000-30000', 24), ('40000-50000', 2083), ('10000-20000', 94), ('60000-70000', 442), ('30000-40000', 1296), ('90000-100000', 58), ('70000-80000', 243), ('80000-90000', 74)]\n",
        "linear price lables: [(100000, 54), (25000, 24), (65000, 442), (45000, 2083), (55000, 1013), (95000, 58), (15000, 94), (5000, 8), (85000, 74), (35000, 1296), (75000, 243)]\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (5389, 13660)\n",
        "vector y dimensions:5389\n",
        "sentiment vector dimensions:5389\n",
        "cross validation accuarcy(Logistic Regression):0.4217089337"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "compare different preprocessing decisions(min_df):\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (5389, 13660)\n",
        "X dimensions="
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " (5389, 13660)\n"
       ]
      },
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-e85daffaafa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m \u001b[0mcompare_mindf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'compare different preprocessing decisions(ngrams):'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-1-e85daffaafa2>\u001b[0m in \u001b[0;36mcompare_mindf\u001b[0;34m(reviews, y)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0maccuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfreq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bo-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-1-e85daffaafa2>\u001b[0m in \u001b[0;36mdo_cv\u001b[0;34m(X, y, nfolds)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'cross validation accuarcy(Logistic Regression):%.10f'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mdo_cv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, score_func, pre_dispatch)\u001b[0m\n\u001b[1;32m   1149\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1151\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1152\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpre_dispatch\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"all\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \"\"\"\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_verbosity_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, args, kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/cross_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters)\u001b[0m\n\u001b[1;32m   1237\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1240\u001b[0m     \u001b[0mtest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/sklearn/svm/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    701\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m                                               rnd.randint(np.iinfo('i').max))\n\u001b[0m\u001b[1;32m    704\u001b[0m         \u001b[0;31m# Regarding rnd.randint(..) in the above signature:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'possible best cross validation accuarcy(Logistic Regression):%.10f'%do_cv(tokenize(review_data, TfidfVectorizer(max_df=0.1, use_idf=True, min_df=1, ngram_range=(1,1), binary=True)), price_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X dimensions= (5389, 13539)\n",
        "best cross validation accuarcy(Logistic Regression):0.4286348219"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(review_data)\n",
      "print len(price_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5389\n",
        "5389\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "price_range = ['0-10000','10000-20000','20000-30000','30000-40000','40000-50000','50000-60000','60000-70000','70000-80000','80000-90000','90000-100000','>100000']\n",
      "\n",
      "for j in range(len(price_range)):\n",
      "    words = []\n",
      "    for i in range(len(review_data)):\n",
      "        if price_data[i] == price_range[j]:\n",
      "            word = review_data[i].split()\n",
      "            for t in word:\n",
      "                if t in afinn:\n",
      "                    words.append(t)\n",
      "    print price_range[j]\n",
      "    print 'most common words:',Counter(words).most_common(30)\n",
      "    \n",
      "filtered_review = []\n",
      "for t in review_data:\n",
      "    words = t.split()\n",
      "    flag = False\n",
      "    for term in words:\n",
      "        if term in afinn:\n",
      "            flag = True\n",
      "    if flag == True:\n",
      "        filtered_review.append(t)\n",
      "print 'raw review:',len(review_data)\n",
      "print 'filtered review:',len(filtered_review)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0-10000\n",
        "most common words: [(u'anxiety', 5), (u'like', 5), (u'perfect', 4), (u'robs', 4), (u'recommend', 4), (u'boost', 4), (u'smart', 4), (u'problem', 4), (u'fantastic', 1), (u'love', 1), (u'want', 1), (u'pleased', 1), (u'best', 1), (u'no', 1), (u'better', 1), (u'good', 1), (u'free', 1), (u'wrong', 1), (u'excellent', 1), (u'loss', 1), (u'lost', 1), (u'solid', 1), (u'appreciate', 1)]\n",
        "10000-20000\n",
        "most common words: [(u'like', 34), (u'love', 26), (u'great', 20), (u'no', 17), (u'better', 13), (u'good', 13), (u'fun', 10), (u'problems', 10), (u'happy', 9), (u'want', 8), (u'hard', 7), (u'nice', 7), (u'best', 6), (u'increase', 6), (u'improved', 6), (u'recommend', 6), (u'pretty', 6), (u'solid', 6), (u'amazing', 6), (u'impressive', 5), (u'excellent', 5), (u'perfect', 4), (u'screaming', 4), (u'easy', 4), (u'enjoying', 4), (u'worst', 4), (u'skeptical', 4), (u'feeling', 4), (u'fake', 4), (u'accident', 4)]\n",
        "20000-30000\n",
        "most common words: [(u'great', 8), (u'love', 5), (u'better', 5), (u'like', 5), (u'no', 4), (u'regret', 3), (u'best', 3), (u'fit', 3), (u'good', 3), (u'excellent', 3), (u'charges', 3), (u'tired', 2), (u'skeptical', 2), (u'amazing', 2), (u'easy', 2), (u'pretty', 2), (u'big', 2), (u'free', 2), (u'trouble', 2), (u'convinced', 2), (u'problem', 2), (u'drop', 2), (u'stronger', 2), (u'fun', 2), (u'limited', 1), (u'hard', 1), (u'satisfied', 1), (u'comfortable', 1), (u'stop', 1), (u'shame', 1)]\n",
        "30000-40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 496), (u'no', 335), (u'great', 286), (u'good', 262), (u'better', 224), (u'love', 220), (u'fun', 164), (u'comfortable', 132), (u'nice', 129), (u'best', 113), (u'want', 103), (u'problems', 89), (u'hard', 87), (u'easy', 86), (u'problem', 76), (u'fit', 74), (u'excellent', 69), (u'happy', 68), (u'pretty', 66), (u'big', 59), (u'recommend', 56), (u'stop', 49), (u'bad', 46), (u'impressed', 43), (u'liked', 42), (u'huge', 40), (u'poor', 37), (u'enjoy', 33), (u'uncomfortable', 32), (u'lack', 31)]\n",
        "40000-50000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 651), (u'great', 510), (u'no', 461), (u'love', 436), (u'good', 414), (u'better', 374), (u'comfortable', 243), (u'fun', 238), (u'best', 173), (u'problem', 150), (u'nice', 144), (u'easy', 135), (u'want', 121), (u'happy', 119), (u'big', 105), (u'problems', 105), (u'hard', 104), (u'pretty', 101), (u'top', 96), (u'excellent', 94), (u'bad', 92), (u'fit', 90), (u'impressed', 81), (u'comfort', 76), (u'loved', 74), (u'recommend', 66), (u'enjoy', 63), (u'perfect', 62), (u'stop', 61), (u'solid', 56)]\n",
        "50000-60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 325), (u'no', 251), (u'great', 209), (u'love', 172), (u'better', 157), (u'good', 153), (u'comfortable', 96), (u'problems', 94), (u'nice', 88), (u'fun', 83), (u'best', 80), (u'problem', 74), (u'happy', 70), (u'want', 69), (u'hard', 66), (u'top', 48), (u'bad', 48), (u'fit', 45), (u'big', 41), (u'loved', 40), (u'stop', 40), (u'poor', 37), (u'comfort', 32), (u'recommend', 31), (u'safety', 31), (u'worth', 30), (u'enjoy', 29), (u'easy', 29), (u'pretty', 28), (u'excellent', 27)]\n",
        "60000-70000\n",
        "most common words: [(u'like', 153), (u'no', 125), (u'great', 100), (u'love', 81), (u'better', 61), (u'good', 54), (u'problem', 54), (u'nice', 40), (u'problems', 38), (u'fun', 37), (u'best', 36), (u'want', 33), (u'comfortable', 31), (u'bad', 25), (u'fit', 23), (u'top', 22), (u'big', 19), (u'hard', 19), (u'happy', 19), (u'loved', 18), (u'excellent', 17), (u'beautiful', 16), (u'poor', 15), (u'stop', 14), (u'hope', 13), (u'stuck', 12), (u'easy', 12), (u'comfort', 12), (u'worst', 11), (u'safety', 11)]\n",
        "70000-80000\n",
        "most common words: [(u'like', 102), (u'no', 60), (u'best', 30), (u'fun', 30), (u'comfortable', 30), (u'want', 30), (u'love', 28), (u'good', 27), (u'great', 27), (u'top', 23), (u'easy', 19), (u'solid', 18), (u'superior', 14), (u'better', 14), (u'aggressive', 13), (u'big', 13), (u'excellent', 13), (u'poor', 13), (u'enjoy', 12), (u'perfect', 11), (u'nice', 11), (u'recommend', 11), (u'lost', 10), (u'happy', 10), (u'problem', 10), (u'responsive', 9), (u'fine', 9), (u'save', 9), (u'liked', 9), (u'feeling', 9)]\n",
        "80000-90000\n",
        "most common words: [(u'like', 29), (u'no', 27), (u'better', 26), (u'great', 18), (u'comfortable', 15), (u'love', 14), (u'good', 13), (u'fun', 13), (u'nice', 11), (u'excellent', 11), (u'easy', 9), (u'problems', 8), (u'super', 8), (u'fit', 7), (u'top', 5), (u'advanced', 5), (u'seduced', 4), (u'hard', 4), (u'ruin', 4), (u'want', 4), (u'substantially', 4), (u'aggressive', 4), (u'best', 4), (u'accomplished', 4), (u'superior', 4), (u'grand', 4), (u'feeling', 4), (u'intelligent', 4), (u'enjoy', 4), (u'advantage', 4)]\n",
        "90000-100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 17), (u'no', 15), (u'outstanding', 12), (u'easy', 10), (u'comfort', 8), (u'comforting', 8), (u'fine', 8), (u'worth', 8), (u'care', 8), (u'best', 6), (u'good', 5), (u'enjoy', 4), (u'blind', 4), (u'liked', 4), (u'win', 4), (u'matter', 4), (u'dead', 4), (u'cries', 4), (u'anti', 4), (u'fail', 4), (u'clarity', 4), (u'hate', 4), (u'pleased', 4), (u'impressed', 4), (u'top', 4), (u'superb', 4), (u'better', 4), (u'recommended', 4), (u'big', 4), (u'refused', 4)]\n",
        ">100000\n",
        "most common words: [(u'like', 25), (u'best', 19), (u'love', 16), (u'great', 11), (u'no', 9), (u'better', 7), (u'clearly', 6), (u'prepared', 6), (u'excellent', 6), (u'solid', 6), (u'pay', 6), (u'powerful', 5), (u'careful', 5), (u'good', 5), (u'backing', 4), (u'alive', 4), (u'cancel', 4), (u'strong', 4), (u'top', 4), (u'recommend', 4), (u'lowest', 4), (u'lucky', 4), (u'warmth', 4), (u'admit', 4), (u'problem', 4), (u'happy', 4), (u'hard', 3), (u'improvement', 3), (u'easy', 3), (u'active', 2)]\n",
        "raw review:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5389\n",
        "filtered review: 5249\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "name": "",
  "signature": "sha256:0e0d4809394089be4e7c1a9c62f19114734c103a2876de17e5209bfc772c5435"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.cross_validation import KFold\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.linear_model import LinearRegression \n",
      "from collections import Counter\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.pipeline import Pipeline\n",
      "from pprint import pprint\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy as np\n",
      "import math\n",
      "import glob\n",
      "import re\n",
      "import time\n",
      "\n",
      "#sentiment afinn\n",
      "afinn_file = open('AFINN/AFINN-111.txt')\n",
      "\n",
      "afinn = dict()\n",
      "\n",
      "for line in afinn_file:\n",
      "    parts = line.strip().split()\n",
      "    if len(parts) == 2:\n",
      "        afinn[parts[0]] = int(parts[1])\n",
      "\n",
      "def afinn_sentiment(terms, afinn):\n",
      "    total = 0.\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            #print '\\t%s=%d' % (t, afinn[t])\n",
      "            total += afinn[t]\n",
      "    return total\n",
      "\n",
      "def afinn_sentiment2(terms, afinn, verbose=False):\n",
      "    pos = 0\n",
      "    neg = 0\n",
      "    for t in terms:\n",
      "        if t in afinn:\n",
      "            if verbose:\n",
      "                print '\\t%s=%d' % (t, afinn[t])\n",
      "            if afinn[t] > 0:\n",
      "                pos += afinn[t]\n",
      "            else:\n",
      "                neg += -1 * afinn[t]\n",
      "    return pos, neg\n",
      "\n",
      "start_time = time.time()\n",
      "filename = \"cars\"\n",
      "\n",
      "def read_automobiles(filename):\n",
      "    \"\"\" Read a list of automobiles.\n",
      "    Args:\n",
      "      filename: The name of the text file containing automobiles make model year.\n",
      "    Returns:\n",
      "      A list of automobile make model year\n",
      "    \"\"\"\n",
      "    automobiles = []\n",
      "    with open(filename) as f:\n",
      "        lines = f.read().splitlines()\n",
      "        for item in lines:\n",
      "            automobiles.append(item)  \n",
      "    return automobiles\n",
      "\n",
      "autos = read_automobiles(filename+'.txt')\n",
      "review_data = [] #X\n",
      "price_data = [] #y\n",
      "price_data_linear = []#y linear\n",
      "sentiment_data = [] #sentiment\n",
      "sentiment_data_linear = [] #sentiment linear\n",
      "\n",
      "#collect data\n",
      "for auto in autos:\n",
      "    filelist = []\n",
      "    filelist = glob.glob(filename + \"_review/\"+auto+\"/*.txt\")\n",
      "    tcofile = glob.glob(filename + \"_TCO/\" + filename + \"_TCO.txt\")\n",
      "    for fil in filelist:\n",
      "        with open(tcofile[0],'r') as file2:\n",
      "            with open(fil,'r') as file1:\n",
      "                price = 0\n",
      "                linear_price = 0\n",
      "                for line in file2:\n",
      "                    if line.split()[3] == fil.split(\"/\")[2][:9]:\n",
      "                        if float(line.split()[4]) >= 0 and float(line.split()[4])<10000:\n",
      "                            price = \"0-10000\"\n",
      "                            linear_price = 5000\n",
      "                        if float(line.split()[4]) >= 10000 and float(line.split()[4])<20000:\n",
      "                            price = \"10000-20000\"\n",
      "                            linear_price = 15000\n",
      "                        if float(line.split()[4]) >= 20000 and float(line.split()[4])<30000:\n",
      "                            price = \"20000-30000\"\n",
      "                            linear_price = 25000\n",
      "                        if float(line.split()[4]) >= 30000 and float(line.split()[4])<40000:\n",
      "                            price = \"30000-40000\"\n",
      "                            linear_price = 35000\n",
      "                        if float(line.split()[4]) >= 40000 and float(line.split()[4])<50000:\n",
      "                            price = \"40000-50000\"\n",
      "                            linear_price = 45000\n",
      "                        if float(line.split()[4]) >= 50000 and float(line.split()[4])<60000:\n",
      "                            price = \"50000-60000\"\n",
      "                            linear_price = 55000\n",
      "                        if float(line.split()[4]) >= 60000 and float(line.split()[4])<70000:\n",
      "                            price = \"60000-70000\"\n",
      "                            linear_price = 65000\n",
      "                        if float(line.split()[4]) >= 70000 and float(line.split()[4])<80000:\n",
      "                            price = \"70000-80000\"\n",
      "                            linear_price = 75000\n",
      "                        if float(line.split()[4]) >= 80000 and float(line.split()[4])<90000:\n",
      "                            price = \"80000-90000\"\n",
      "                            linear_price = 85000\n",
      "                        if float(line.split()[4]) >= 90000 and float(line.split()[4])<100000:\n",
      "                            price = \"90000-100000\"\n",
      "                            linear_price = 95000\n",
      "                        if float(line.split()[4]) >= 100000:\n",
      "                            price = \">100000\"\n",
      "                            linear_price = 100000\n",
      "\n",
      "                        reviews = file1.read().split('\\n------------------------------------------------------------------------------------------\\n')\n",
      "                        reviews = filter(None,reviews)\n",
      "                        for i in range(0,len(reviews)):\n",
      "                            review_data.append(reviews[i].replace(\"\\n\",\"\").decode('UTF-8'))\n",
      "                            price_data_linear.append(linear_price)\n",
      "                            price_data.append(price)\n",
      "                            sentiment_data.append(afinn_sentiment(reviews[i].replace(\"\\n\",\"\").decode('UTF-8').split(), afinn))\n",
      "                            sentiment_data_linear.append(afinn_sentiment2(reviews[i].replace(\"\\n\",\"\").decode('UTF-8').split(), afinn))\n",
      "                            \n",
      "print 'sentiment vector1:',Counter(sentiment_data).items()\n",
      "print 'price labels:', Counter(price_data).items()\n",
      "print 'linear price lables:',Counter(price_data_linear).items()\n",
      "\n",
      "\n",
      "def tokenize(reviews,vectorizer):\n",
      "    X = vectorizer.fit_transform(reviews)\n",
      "    print 'X dimensions=', X.shape\n",
      "    return X\n",
      "\n",
      "X = tokenize(review_data, TfidfVectorizer())\n",
      "print 'vector y dimensions:%d'%len(price_data)\n",
      "print 'sentiment vector dimensions:%d'%len(sentiment_data)\n",
      "\n",
      "def do_cv(X, y, nfolds=10):\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    return np.mean(cross_val_score(LogisticRegression(), X, y, cv=cv))\n",
      "\n",
      "def do_cv_linear(X, y, nfolds=10):\n",
      "    cv = KFold(len(y), nfolds)\n",
      "    return np.mean(cross_val_score(LinearRegression(), X, y, cv=cv))\n",
      "\n",
      "print 'cross validation accuarcy(Logistic Regression):%.10f'%do_cv(X, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(min_df):'\n",
      "\n",
      "#how filtering rare words affect accuracy\n",
      "best_min_df = 0\n",
      "def compare_mindf(reviews, y):\n",
      "    accuracies = []\n",
      "    for freq in range(20):\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(min_df=freq)), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xlabel('mindf')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "    \n",
      "compare_mindf(review_data,price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(ngrams):'\n",
      "best_ngram_range = (1,1)\n",
      "def compare_ngrams(reviews, y):\n",
      "    accuracies = []\n",
      "    ngrams = [(1,1), (1,2), (2,2), (1,3), (2,3), (3,3), (3,4), (4,4)]\n",
      "    for ngram in ngrams:\n",
      "        print ngram\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(min_df=2, ngram_range=ngram)), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks(range(len(ngrams)), ngrams)\n",
      "    plt.xlabel('ngrams')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "   \n",
      "compare_ngrams(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(binary/freq):'\n",
      "best_binary = True\n",
      "def compare_binary(reviews, y):\n",
      "    accuracies = []\n",
      "    choices = [True, False]\n",
      "    for choice in choices:\n",
      "        print choice\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(binary=choice, min_df=2,ngram_range=(1,3))), y))\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks([0,1], [str(c) for c in choices])\n",
      "    plt.xlabel('binary')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "        \n",
      "compare_binary(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(idf):'\n",
      "best_use_idf= True\n",
      "def compare_idf(reviews, y):\n",
      "    accuracies = []\n",
      "    choices = [True, False]\n",
      "    for choice in choices:\n",
      "        print choice\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(binary=True,min_df=2,ngram_range=(1,3),use_idf=choice)), y))\n",
      "\n",
      "    plt.figure()\n",
      "    plt.plot(accuracies, 'bo-')\n",
      "    plt.xticks([0,1], [str(c) for c in choices])\n",
      "    plt.xlabel('binary')\n",
      "    plt.ylabel('use idf?')\n",
      "    plt.show()\n",
      "        \n",
      "compare_idf(review_data, price_data)\n",
      "\n",
      "print 'compare different preprocessing decisions(filtering common words affect):'\n",
      "best_maxdf=1.\n",
      "def compare_maxdf(reviews, y):\n",
      "    accuracies = []\n",
      "    maxdfs = [1., .1, .05, .04, .03, .02, .01,.005,.001]\n",
      "    for freq in maxdfs:\n",
      "        print freq\n",
      "        accuracies.append(do_cv(tokenize(reviews, TfidfVectorizer(max_df=freq)), y))\n",
      "        \n",
      "    plt.figure()\n",
      "    plt.plot(maxdfs, accuracies, 'bo-')\n",
      "    plt.xlabel('maxdf (fraction)')\n",
      "    plt.ylabel('accuracy')\n",
      "    plt.show()\n",
      "        \n",
      "compare_maxdf(review_data, price_data)\n",
      "\n",
      "print 'GridSearch on all parameters:'\n",
      "pipeline = Pipeline([\n",
      "    ('vect', TfidfVectorizer()),\n",
      "    ('clf', LogisticRegression()),\n",
      "])\n",
      "\n",
      "parameters = {\n",
      "    'vect__min_df': (1, 2, 3, 4, 5),\n",
      "    'vect__ngram_range': ((1, 1), (1, 2), (1, 3)),  # unigrams or bigrams or trigrams\n",
      "    'vect__use_idf': (True, False),\n",
      "}\n",
      "grid_search = GridSearchCV(pipeline, parameters, verbose=1, cv=KFold(len(price_data), 10))\n",
      "print \"Performing grid search...\"\n",
      "print \"pipeline:\", [name for name, _ in pipeline.steps]\n",
      "print \"parameters:\"\n",
      "pprint(parameters)\n",
      "grid_search.fit(review_data, price_data)\n",
      "print \"done.\"\n",
      "\n",
      "print(\"Best accuarcy: %0.3f\" % grid_search.best_score_)\n",
      "print(\"Best parameters set:\")\n",
      "best_parameters = grid_search.best_estimator_.get_params()\n",
      "for param_name in sorted(parameters.keys()):\n",
      "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "print \"\\nRunning time is %s seconds\"%(time.time()-start_time)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print 'possible best cross validation accuarcy(Logistic Regression):%.10f'%do_cv(tokenize(review_data, TfidfVectorizer(max_df=0.1, use_idf=True, min_df=1, ngram_range=(1,1), binary=True)), price_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "X dimensions= (5389, 13539)\n",
        "best cross validation accuarcy(Logistic Regression):0.4286348219"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(review_data)\n",
      "print len(price_data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "5389\n",
        "5389\n"
       ]
      }
     ],
     "prompt_number": 51
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "price_range = ['0-10000','10000-20000','20000-30000','30000-40000','40000-50000','50000-60000','60000-70000','70000-80000','80000-90000','90000-100000','>100000']\n",
      "\n",
      "for j in range(len(price_range)):\n",
      "    words = []\n",
      "    for i in range(len(review_data)):\n",
      "        if price_data[i] == price_range[j]:\n",
      "            word = review_data[i].split()\n",
      "            for t in word:\n",
      "                if t in afinn:\n",
      "                    words.append(t)\n",
      "    print price_range[j]\n",
      "    print 'most common words:',Counter(words).most_common(30)\n",
      "    \n",
      "filtered_review = []\n",
      "for t in review_data:\n",
      "    words = t.split()\n",
      "    flag = False\n",
      "    for term in words:\n",
      "        if term in afinn:\n",
      "            flag = True\n",
      "    if flag == True:\n",
      "        filtered_review.append(t)\n",
      "print 'raw review:',len(review_data)\n",
      "print 'filtered review:',len(filtered_review)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0-10000\n",
        "most common words: [(u'anxiety', 5), (u'like', 5), (u'perfect', 4), (u'robs', 4), (u'recommend', 4), (u'boost', 4), (u'smart', 4), (u'problem', 4), (u'fantastic', 1), (u'love', 1), (u'want', 1), (u'pleased', 1), (u'best', 1), (u'no', 1), (u'better', 1), (u'good', 1), (u'free', 1), (u'wrong', 1), (u'excellent', 1), (u'loss', 1), (u'lost', 1), (u'solid', 1), (u'appreciate', 1)]\n",
        "10000-20000\n",
        "most common words: [(u'like', 34), (u'love', 26), (u'great', 20), (u'no', 17), (u'better', 13), (u'good', 13), (u'fun', 10), (u'problems', 10), (u'happy', 9), (u'want', 8), (u'hard', 7), (u'nice', 7), (u'best', 6), (u'increase', 6), (u'improved', 6), (u'recommend', 6), (u'pretty', 6), (u'solid', 6), (u'amazing', 6), (u'impressive', 5), (u'excellent', 5), (u'perfect', 4), (u'screaming', 4), (u'easy', 4), (u'enjoying', 4), (u'worst', 4), (u'skeptical', 4), (u'feeling', 4), (u'fake', 4), (u'accident', 4)]\n",
        "20000-30000\n",
        "most common words: [(u'great', 8), (u'love', 5), (u'better', 5), (u'like', 5), (u'no', 4), (u'regret', 3), (u'best', 3), (u'fit', 3), (u'good', 3), (u'excellent', 3), (u'charges', 3), (u'tired', 2), (u'skeptical', 2), (u'amazing', 2), (u'easy', 2), (u'pretty', 2), (u'big', 2), (u'free', 2), (u'trouble', 2), (u'convinced', 2), (u'problem', 2), (u'drop', 2), (u'stronger', 2), (u'fun', 2), (u'limited', 1), (u'hard', 1), (u'satisfied', 1), (u'comfortable', 1), (u'stop', 1), (u'shame', 1)]\n",
        "30000-40000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 496), (u'no', 335), (u'great', 286), (u'good', 262), (u'better', 224), (u'love', 220), (u'fun', 164), (u'comfortable', 132), (u'nice', 129), (u'best', 113), (u'want', 103), (u'problems', 89), (u'hard', 87), (u'easy', 86), (u'problem', 76), (u'fit', 74), (u'excellent', 69), (u'happy', 68), (u'pretty', 66), (u'big', 59), (u'recommend', 56), (u'stop', 49), (u'bad', 46), (u'impressed', 43), (u'liked', 42), (u'huge', 40), (u'poor', 37), (u'enjoy', 33), (u'uncomfortable', 32), (u'lack', 31)]\n",
        "40000-50000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 651), (u'great', 510), (u'no', 461), (u'love', 436), (u'good', 414), (u'better', 374), (u'comfortable', 243), (u'fun', 238), (u'best', 173), (u'problem', 150), (u'nice', 144), (u'easy', 135), (u'want', 121), (u'happy', 119), (u'big', 105), (u'problems', 105), (u'hard', 104), (u'pretty', 101), (u'top', 96), (u'excellent', 94), (u'bad', 92), (u'fit', 90), (u'impressed', 81), (u'comfort', 76), (u'loved', 74), (u'recommend', 66), (u'enjoy', 63), (u'perfect', 62), (u'stop', 61), (u'solid', 56)]\n",
        "50000-60000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 325), (u'no', 251), (u'great', 209), (u'love', 172), (u'better', 157), (u'good', 153), (u'comfortable', 96), (u'problems', 94), (u'nice', 88), (u'fun', 83), (u'best', 80), (u'problem', 74), (u'happy', 70), (u'want', 69), (u'hard', 66), (u'top', 48), (u'bad', 48), (u'fit', 45), (u'big', 41), (u'loved', 40), (u'stop', 40), (u'poor', 37), (u'comfort', 32), (u'recommend', 31), (u'safety', 31), (u'worth', 30), (u'enjoy', 29), (u'easy', 29), (u'pretty', 28), (u'excellent', 27)]\n",
        "60000-70000\n",
        "most common words: [(u'like', 153), (u'no', 125), (u'great', 100), (u'love', 81), (u'better', 61), (u'good', 54), (u'problem', 54), (u'nice', 40), (u'problems', 38), (u'fun', 37), (u'best', 36), (u'want', 33), (u'comfortable', 31), (u'bad', 25), (u'fit', 23), (u'top', 22), (u'big', 19), (u'hard', 19), (u'happy', 19), (u'loved', 18), (u'excellent', 17), (u'beautiful', 16), (u'poor', 15), (u'stop', 14), (u'hope', 13), (u'stuck', 12), (u'easy', 12), (u'comfort', 12), (u'worst', 11), (u'safety', 11)]\n",
        "70000-80000\n",
        "most common words: [(u'like', 102), (u'no', 60), (u'best', 30), (u'fun', 30), (u'comfortable', 30), (u'want', 30), (u'love', 28), (u'good', 27), (u'great', 27), (u'top', 23), (u'easy', 19), (u'solid', 18), (u'superior', 14), (u'better', 14), (u'aggressive', 13), (u'big', 13), (u'excellent', 13), (u'poor', 13), (u'enjoy', 12), (u'perfect', 11), (u'nice', 11), (u'recommend', 11), (u'lost', 10), (u'happy', 10), (u'problem', 10), (u'responsive', 9), (u'fine', 9), (u'save', 9), (u'liked', 9), (u'feeling', 9)]\n",
        "80000-90000\n",
        "most common words: [(u'like', 29), (u'no', 27), (u'better', 26), (u'great', 18), (u'comfortable', 15), (u'love', 14), (u'good', 13), (u'fun', 13), (u'nice', 11), (u'excellent', 11), (u'easy', 9), (u'problems', 8), (u'super', 8), (u'fit', 7), (u'top', 5), (u'advanced', 5), (u'seduced', 4), (u'hard', 4), (u'ruin', 4), (u'want', 4), (u'substantially', 4), (u'aggressive', 4), (u'best', 4), (u'accomplished', 4), (u'superior', 4), (u'grand', 4), (u'feeling', 4), (u'intelligent', 4), (u'enjoy', 4), (u'advantage', 4)]\n",
        "90000-100000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "most common words: [(u'like', 17), (u'no', 15), (u'outstanding', 12), (u'easy', 10), (u'comfort', 8), (u'comforting', 8), (u'fine', 8), (u'worth', 8), (u'care', 8), (u'best', 6), (u'good', 5), (u'enjoy', 4), (u'blind', 4), (u'liked', 4), (u'win', 4), (u'matter', 4), (u'dead', 4), (u'cries', 4), (u'anti', 4), (u'fail', 4), (u'clarity', 4), (u'hate', 4), (u'pleased', 4), (u'impressed', 4), (u'top', 4), (u'superb', 4), (u'better', 4), (u'recommended', 4), (u'big', 4), (u'refused', 4)]\n",
        ">100000\n",
        "most common words: [(u'like', 25), (u'best', 19), (u'love', 16), (u'great', 11), (u'no', 9), (u'better', 7), (u'clearly', 6), (u'prepared', 6), (u'excellent', 6), (u'solid', 6), (u'pay', 6), (u'powerful', 5), (u'careful', 5), (u'good', 5), (u'backing', 4), (u'alive', 4), (u'cancel', 4), (u'strong', 4), (u'top', 4), (u'recommend', 4), (u'lowest', 4), (u'lucky', 4), (u'warmth', 4), (u'admit', 4), (u'problem', 4), (u'happy', 4), (u'hard', 3), (u'improvement', 3), (u'easy', 3), (u'active', 2)]\n",
        "raw review:"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 5389\n",
        "filtered review: 5249\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}